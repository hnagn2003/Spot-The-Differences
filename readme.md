# BÃ i táº­p Xá»­ LÃ½ áº¢nh giá»¯a ká»³ - UET

# Link Github

https://github.com/hnagn2003/Spot-The-Differences

## TÃ­nh nÄƒng

- Tá»« áº£nh input, generate áº£nh cÃ³ má»™t sá»‘ Ä‘iá»ƒm sai khÃ¡c vá»›i áº£nh gá»‘c phá»¥c vá»¥ game Find The Differences.
- Tá»« hai áº£nh, chá»‰ ra (khoanh trÃ²n) cÃ¡c Ä‘iá»ƒm khÃ¡c nhau.

![result.png](readme/result.png)

# 1. Level 1 + 2

Level 1 vÃ  2 sáº½ hoáº¡t Ä‘á»™ng trÃªn áº£nh animated, mang tÃ­nh Ä‘Æ¡n giáº£n

## 1.1. Pipeline

ðŸ’¡ Ã tÆ°á»Ÿng thuáº­t toÃ¡n: Segment input image Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c objects, tá»« Ä‘Ã³ thá»±c hiá»‡n cÃ¡c thay Ä‘á»•i Ä‘á»‘i vá»›i objects Ä‘á»ƒ táº¡o ra áº£nh cÃ³ cÃ¡c Ä‘iá»ƒm khÃ¡c vá»›i áº£nh gá»‘c. 

### 1.1.1. Input, Output

ThÃ´ng qua terminal cÃ³ thá»ƒ config cÃ¡c thÃ´ng sá»‘:

- level: Ä‘á»™ khÃ³ trÃ² chÆ¡i, 1 hoáº·c 2. Level cÃ ng cao, object cÃ ng nhá».
- input image: path
- output: output folder
- nums_of_output: sá»‘ lÆ°á»£ng output cáº§n generate, máº·c Ä‘á»‹nh má»—i láº§n generate sáº½ táº¡o ra 3 áº£nh
- nums_of_spots: sá»‘ Ä‘iá»ƒm sai khÃ¡c táº¡o ra so vá»›i áº£nh gá»‘c

```python
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--level',
        default=1,
        help='level of generate')
    parser.add_argument(
        '--input_image',
        default='input/input_image.png',
        help='input image file/url')
    parser.add_argument(
        '--output',
        default='output',
        help='output image file/url')
    parser.add_argument(
        '--nums_of_output',
        default=3,
        help='number of output'
    )
    parser.add_argument(
        '--nums_of_spots',
        default=3,
        help='number of spots'
    )
    args = parser.parse_args()
    return args
```

Äá»c áº£nh input/input_image.png:

```jsx
input_img = cv2.imread(args.input_image)
```

![input_image.png](readme/input_image.png)

### 1.1.2. Detect objects

Tá»« input, ta sáº½ thá»±c hiá»‡n viá»‡c segment báº±ng thÆ° viá»‡n pymeanshift

```jsx
labels_image, objects = segment(input_img, args.level)
```

Pymeanshift algorithm:
- Input: Image
- Output: CÃ¡c objects, Ä‘Æ°á»£c Ä‘áº¡i diá»‡n báº±ng label (id), centroid, area, bounding box, mean_intensity, â€¦
- Má»¥c Ä‘Ã­ch: Detect cÃ¡c objects trong áº£nh, phá»¥c vá»¥ viá»‡c remove, change color, â€¦
- Ã tÆ°á»Ÿng thuáº­t toÃ¡n: Clustering: chá»n Ä‘iá»ƒm áº£nh ngáº«u nhiÃªn, sau Ä‘Ã³ tÃ¬m táº¥t cáº£ cÃ¡c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng á»Ÿ gáº§n nÃ³ trong bandwidth. Sau Ä‘Ã³ tÃ¬m cluster center má»›i, cá»© tháº¿ láº·p Ä‘i láº·p láº¡i Ä‘á»ƒ phÃ¡t hiá»‡n ra segment.

```python
def segment(img, level):
    spatial_radius=6
    range_radius=4.5
    min_density=50

    (segmented_image, labels_image, number_regions) = pms.segment(img,
                                                                spatial_radius,
                                                                range_radius,
                                                                min_density
                                                    )

    props = measure.regionprops(labels_image, intensity_image=img)

    # filter regions based on size and shape
    min_area = 100  # minimum area of an object in pixels
    max_area = 100000
    max_eccentricity = 0.8  # maximum eccentricity of an object
    if (level == 1):
        min_area = 50  # minimum area of an object in pixels
        max_area = 100000
        max_eccentricity = 0.8  # maximum eccentricity of an object
    if (level == 2):
        min_area = 25  # minimum area of an object in pixels
        max_area = 100
        max_eccentricity = 0.8  # maximum eccentricity of an object
        
    objects = []
    for i, prop in enumerate(props):
        if prop.area >= min_area and prop.area <= max_area and prop.eccentricity <= max_eccentricity:
            objects.append({
                'label': i,
                'area': prop.area,
                'centroid': prop.centroid,
                'bbox': prop.bbox,
                'mean_intensity': prop.mean_intensity,
                'min_intensity': prop.min_intensity,
                'max_intensity': prop.max_intensity
            })

    display or use the object information
    for obj in objects:
        print(f"Object {obj['label']}: area = {obj['area']}, centroid = {obj['centroid']}, mean intensity = {obj['mean_intensity']}, bbox = {obj['bbox']}")
    return labels_image, objects
```

![Untitled](readme/Untitled.png)

![Untitled](readme/Untitled%201.png)

á»ž bÃªn trÃªn, labels_image lÃ  má»™t ma tráº­n Ä‘áº¡i diá»‡n cho cÃ¡c objects, cÃ³ cÃ¹ng size vá»›i áº£nh input. Cá»¥ thá»ƒ, má»—i má»™t pháº§n tá»­ trong labels_image sáº½ lÃ  label cá»§a chÃ­nh object tÆ°Æ¡ng á»©ng vá»›i nÃ³ trong áº£nh gá»‘c. CÃ¹ng má»™t objects sáº½ Ä‘Æ°á»£c Ä‘áº¡i diá»‡n báº±ng cÃ¡c label báº±ng nhau. CÃ¡i nÃ y Ä‘Æ°á»£c gá»i lÃ  segmentation mask.

### 1.1.3. Remove, Change Color

Äá»ƒ táº¡o ra cÃ¡c Ä‘iá»ƒm khÃ¡c so vá»›i áº£nh gá»‘c, ta cÃ³ thá»ƒ remove hoáº·c change color cá»§a object

Viá»‡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch thay tháº¿ ma tráº­n object thÃ nh ma tráº­n mÃ u background (remove) hoáº·c mÃ u tÃ¹y Ã½ (change color). 

```python
for i, obj in enumerate (random_objs):
        label_to_change = obj['label']
        modify = random_modify[i]
        object_color = background_color if modify == 0 else get_random_color()

        # Set the pixel values of the object to the desired color using NumPy indexing
        output_img[labels_image == (label_to_change+1)] = object_color
        img_id = datetime.datetime.now().strftime('%f')

    cv2.imwrite(output_dir+'/'+img_id+'.png', output_img)
```

Ta Ä‘Æ°á»£c output nhÆ° sau:

![output.png](readme/output.png)

So sÃ¡nh vá»›i áº£nh gá»‘c á»Ÿ phÃ­a trÃªn, máº·t trá»i Ä‘Ã£ biáº¿n máº¥t (remove), mÃ¢y, chÃ¢n gháº¿, ngá»n cá» thay mÃ u (change color). 

Viá»‡c chá»n modify (remove hoáº·c change color), viá»‡c chá»n objects Ä‘á»ƒ modify lÃ  ngáº«u nhiÃªn.

## 1.2. Level 1 vs Level 2

Level 1 so vá»›i level 2 chá»‰ khÃ¡c nhau á»Ÿ chá»—, level 1 sáº½ modify cÃ¡c váº­t to (area lá»›n), level 2 modify cÃ¡c váº­t nhá» hÆ¡n.

```python
def segment(img, level):
#...
	min_area = 100  # minimum area of an object in pixels
	    max_area = 100000
	    max_eccentricity = 0.8  # maximum eccentricity of an object
	    if (level == 1):
	        min_area = 50  # minimum area of an object in pixels
	        max_area = 100000
	        max_eccentricity = 0.8  # maximum eccentricity of an object
	    if (level == 2):
	        min_area = 25  # minimum area of an object in pixels
	        max_area = 100
	        max_eccentricity = 0.8  # maximum eccentricity of an object
#...

```

## 1.3. Implementation & Run

- Implementation.
    
    ```python
    git clone https://github.com/hnagn2003/Spot-The-Differences
    cd pymeanshift
    ./setup.py install
    pip install -r requirements.txt
    ```
    

```python
python main.py --level=1
python main.py --level=1 --nums_of_spots=4

python3 find_the_differences.py --img1 ./input/input_image.png --img2  ./output/output.png
python3 find_the_differences.py --img1 input/input_lv3.png --img2  level_3/output.png
```

NhÆ° váº­y, ta Ä‘Ã£ hoÃ n thÃ nh 2 level Ä‘áº§u tiÃªn, vá»›i nhá»¯ng thuáº­t toÃ¡n vÃ´ cÃ¹ng Ä‘Æ¡n giáº£n Ä‘á»ƒ xá»­ lÃ½ viá»‡c generate áº£nh Ä‘á»‘i vá»›i animated image. Váº­y cÃ²n áº£nh thá»±c thÃ¬ sao?

# 2. Level 3

Váº«n lÃ  váº¥n Ä‘á» nÃªu trÃªn, vá»›i input lÃ  áº£nh thá»±c.

Please check my colab and run it with GPU environment:

[Google Colaboratory](https://colab.research.google.com/drive/1s8n5mo4VKDIGOSgCi4Ih0B8GAPGkI6zz?usp=sharing)

## 2.1. Pipeline

Äá»c vÃ o input image

![input_lv3.png](readme/input_lv3.png)

Táº¡o ra cÃ¡c mask báº±ng cÃ¡ch láº¥y generate ngáº«u nhiÃªn cÃ¡c hÃ¬nh trÃ²n cÃ³ radius trong khoáº£ng xÃ¡c Ä‘á»‹nh:

![mask.png](readme/mask.png)

ðŸ’¡ Ã tÆ°á»Ÿng: Sá»­ dá»¥ng pretrained model Stable Diffusion Inpainting (diffusers/clipseg/weights/rd64-uni.pth) Ä‘á»ƒ repaint cÃ¡c vÃ¹ng trong pháº¡m vi vÃ²ng trÃ²n tráº¯ng. Chá»‰ Ä‘Æ¡n giáº£n lÃ  váº­y, ta cÃ³ output: 

![output.png](readme/output%201.png)

![result.png](readme/result%201.png)

ÄÃ³ lÃ  táº¥t cáº£. Check my modified Diffusers here: https://github.com/hnagn2003/diffusers2.

## 2.2. About Stable Diffusions Inpainting

### 2.2.1. Demo

SDI lÃ  má»™t mÃ´ hÃ¬nh tÃ­ch há»£p cáº£ kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. MÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng extract features cá»§a prompt ta nháº­p vÃ o, contribute tá»›i output lÃ  dá»¯ liá»‡u Ä‘Æ°á»£c generate ra trong pháº¡m vi hÃ¬nh trÃ²n tráº¯ng (pháº§n bá»‹ missing, damaged).

![init_image.png](readme/init_image.png)

```python
prompt="pikachu"
output = predict("input/input_lv3.png", "mask.png", prompt)
```

![Untitled](readme/Untitled%202.png)

![Untitled](readme/Untitled%203.png)

CÃ³ thá»ƒ tháº¥y kháº£ nÄƒng recovery áº£nh cá»§a SDI lÃ  khÃ¡ áº¥n tÆ°á»£ng, natural.

- Deployed App:

[Stable Diffusion Inpainting - a Hugging Face Space by multimodalart](https://huggingface.co/spaces/multimodalart/stable-diffusion-inpainting)

- Please take a look of it paper: [https://arxiv.org/abs/2112.10003](https://arxiv.org/abs/2112.10003)
- My modified SDI repo: https://github.com/hnagn2003/Stable-Diffusion-Inpainting

### 2.2.2. Introduction

SDI lÃ  má»™t giáº£i phÃ¡p cho bÃ i toÃ¡n trá»ng yáº¿u Image Segmentation. NÃ³ biáº¿n Ä‘á»•i vÃ¹ng áº£nh Ä‘Æ°á»£c masked dá»±a trÃªn thÃ´ng tin cá»§a image & text prompt.

### 2.2.3. Pipeline

Ã tÆ°á»Ÿng thuáº­t toÃ¡n: Ká»¹ thuáº­t chÃ­nh cá»§a SDI lÃ  Partial Differential Equations. NÃ³ tá»•ng há»£p thÃ´ng tin cá»§a Ä‘áº·c trÆ°ng cá»§a vÃ¹ng áº£nh Ä‘Ã£ biáº¿t, tráº£i qua huáº¥n luyá»‡n vá»›i cÃ¡c prompt, Ä‘á»ƒ generate ra thÃ´ng tin bá»‹ missing.

Kiáº¿n trÃºc transformer-based Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin tá»« image vÃ  text Ä‘á»ƒ táº¡o ra segmentation mask (em Ä‘Ã£ Ä‘á» cáº­p á»Ÿ 1.1.2)

- Visual information Ä‘Æ°á»£c extracted qua máº¡ng CNN
- Textual information: processed using a language model encoder.

2 encoders nÃ y sáº½ Ä‘Æ°á»£c táº­p há»£p qua self-attention

![Untitled](readme/Untitled%204.png)

Model Ä‘Æ°á»£c em sá»­ dá»¥ng á»Ÿ Ä‘Ã¢y lÃ  CLIPSeg (diffusers/clipseg), vá»›i backbone ViT-B/16, pretrained weight (diffusers/clipseg/weights/rd64-uni.pth). 

# 3. Spot the Differences

find_the_differences.py

- Input: 2 images
- Output: mark Ä‘iá»ƒm khÃ¡c biá»‡t giá»¯a 2 images
- Ã tÆ°á»Ÿng thuáº­t toÃ¡n:
    - Input images Ä‘Æ°á»£c convert thÃ nh blurred grayscale, sau Ä‘Ã³ tÃ­nh toÃ¡n differences báº±ng cv2.absdiff().
    
    ```python
    # Convert images to grayscale and blur them
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        blurred1 = cv2.GaussianBlur(gray1, (11, 11), 0)
        blurred2 = cv2.GaussianBlur(gray2, (11, 11), 0)
    
        # Compute absolute difference between the images and threshold the result
        diff = cv2.absdiff(blurred1, blurred2)
    ```
    
    - Tiáº¿p Ä‘Ã³ cv2.threshold() sáº½ tÃ­nh binary mask cá»§a áº£nh (khÃ¡c tráº¯ng giá»‘ng Ä‘en)
    
    ```python
    _, thresh = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)
    ```
    
    - erosion vÃ  dilation Ä‘á»ƒ clean thresholded image vÃ  remove noise
    
    ```python
    # Apply morphological operations to clean up the thresholded image
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        thresh = cv2.erode(thresh, kernel, iterations=2)
        thresh = cv2.dilate(thresh, kernel, iterations=4)
    ```
    
    - Chá»‰ láº¥y cÃ¡c blobs Ä‘Ã¡p á»©ng threshhold
    
    ```python
    # Label and filter blobs in the thresholded image
        labels = measure.label(thresh, connectivity=2, background=0)
        mask = np.zeros(thresh.shape, dtype="uint8")
        for label in np.unique(labels):
            if label == 0:
                continue
            labelMask = np.zeros(thresh.shape, dtype="uint8")
            labelMask[labels == label] = 255
            numPixels = cv2.countNonZero(labelMask)
            if numPixels > min_blob_size:
                mask = cv2.add(mask, labelMask)
    ```
    
    - TÃ¬m contours cá»§a cÃ¡c blobs trÃªn sau Ä‘Ã³ váº½ circles
    
    ```python
    # Find contours of the blobs and draw circles around them on the original images
        cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)
        cnts = contours.sort_contours(cnts)[0]
        for (i, c) in enumerate(cnts):
            (x, y, w, h) = cv2.boundingRect(c)
            ((cX, cY), radius) = cv2.minEnclosingCircle(c)
            cv2.circle(img1, (int(cX), int(cY)), int(radius), (0, 0, 255), 3)
            cv2.circle(img2, (int(cX), int(cY)), int(radius), (0, 0, 255), 3)
        res = np.concatenate((img1, img2), axis=1)
        cv2.imwrite('level_3/result.png', res)
    ```
